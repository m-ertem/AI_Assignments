{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMLLCJ6y7TAL"
      },
      "source": [
        "# Problem Set 1 (Take Home) - 40 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsgxuLig7TAQ"
      },
      "source": [
        "## PS 1.A - 20 points\n",
        "\n",
        "In [this video](https://www.youtube.com/watch?v=ho6JXE3EbZ8) the author explains how to extract various visualizations of what CNNs learn. [Your course site](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/cnn/cnn-example-architectures/visualizing-what-convnets-learn.html) also covers the topic.\n",
        "\n",
        "Using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), train a ResNet-50 based CNN on the classification task of $K=9$ classes (filter out the class `ship`) and create the following visualizations for first, middle and last blocks of ResNet-50. You are free to select a class to showcase such visualizations.\n",
        "\n",
        "* Visualizing intermediate convnet outputs (“intermediate activations”). This is useful to understand how successive convnet layers transform their input.\n",
        "\n",
        "* Visualizing convnets filters. This is useful to understand precisely what visual pattern or concept each filter in a convnet is receptive to.\n",
        "\n",
        "* Visualizing heatmaps of class activation in an image. This is useful to understand which part of an image where identified as belonging to a given class, and thus allows to localize objects in images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have worked with 64x64x3 size images version of CIFAR-10 dataset, because I couldn't fit 32x32x3 image dataset to the model. Also I couldn't filter out ship class because I had a problem with file organization in Colab."
      ],
      "metadata": {
        "id": "qhBkVsgcLuim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modules"
      ],
      "metadata": {
        "id": "tCMj3Gb36FLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXXfvoLH7TAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e7d1fd-074c-4299-d2ed-7c8b286dc699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "#from keras.utils import layer_utils\n",
        "#from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from IPython.display import SVG\n",
        "#from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model, load_img, img_to_array\n",
        "# from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib.pyplot import imshow\n",
        "from keras.backend import mean\n",
        "from keras import backend\n",
        "from tensorflow.compat.v1 import disable_eager_execution, enable_eager_execution\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications import vgg16\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow import gradients\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "import pydot\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define, compile and save the model"
      ],
      "metadata": {
        "id": "R3XDWdPc6Kkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------- Model\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. You'll need this later to add back to the main path.\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path\n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
        "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape=(64, 64, 3), classes=6):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = ResNet50(input_shape = (64, 64, 3), classes = 10)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# save the model\n",
        "model.save('Resnet50.h5')"
      ],
      "metadata": {
        "id": "Xs4lQgdB6AWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7895da-9c3a-4d18-8164-29da3b822f7d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data"
      ],
      "metadata": {
        "id": "Dg0QVt2K6NpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/cifar10-64'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Image pre-processing\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 64x64\n",
        "        target_size=(64, 64),\n",
        "        batch_size=20,\n",
        "        # Since we use sparse_categorical_crossentropy loss\n",
        "        class_mode='sparse')\n",
        "\n",
        "# read test data from file\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        test_dir,\n",
        "        # All images will be resized to 64x64\n",
        "        target_size=(64, 64),\n",
        "        batch_size=20,\n",
        "        # Since we use sparse_categorical_crossentropy loss\n",
        "        class_mode='sparse')\n",
        "\n",
        "# check the data (number of images and classes)\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnK9rjD6eVF",
        "outputId": "19bef79f-b87c-4510-99ea-b1bb1ee1b082"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50000 images belonging to 10 classes.\n",
            "Found 10000 images belonging to 10 classes.\n",
            "data batch shape: (20, 64, 64, 3)\n",
            "labels batch shape: (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit data to the model"
      ],
      "metadata": {
        "id": "UftKaUa_Fn6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit data to the model\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=10,\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6PJyQLRFs_1",
        "outputId": "40763db9-7164-4682-b747-cec2566f3cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-86-e364316d0dfa>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 368s 3s/step - loss: 3.4011 - accuracy: 0.1060\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 304s 3s/step - loss: 3.0324 - accuracy: 0.0935\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 336s 3s/step - loss: 2.6901 - accuracy: 0.1315\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 319s 3s/step - loss: 2.5572 - accuracy: 0.0980\n",
            "Epoch 5/10\n",
            " 45/100 [============>.................] - ETA: 3:08 - loss: 2.3041 - accuracy: 0.0644"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model."
      ],
      "metadata": {
        "id": "5M_bgl0iMl_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read test data from directory into array\n",
        "for i in range(10000):\n",
        "    X_test, y_test = test_generator.next()\n",
        "\n",
        "# y_test is 2-D array so convert it into 1-D array\n",
        "y_test = y_test.reshape(-1,)\n",
        "\n",
        "# test model with test data\n",
        "model.evaluate(X_test, y_test,) # returns loss and accuracy"
      ],
      "metadata": {
        "id": "h3bUXq7xNKTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize intermediate activations"
      ],
      "metadata": {
        "id": "v3knUvS2JJiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()  # As a reminder.\n",
        "\n",
        "# grab an image\n",
        "img_path = '/content/cifar10-64/test/class0/img10.png'\n",
        "\n",
        "# We preprocess the image into a 4D tensor\n",
        "img = load_img(img_path, target_size=(64, 64))\n",
        "img_tensor = img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "# Remember that the model was trained on inputs\n",
        "# that were preprocessed in the following way:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Its shape is (1, 64, 64, 3)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "# show image\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()\n",
        "\n",
        "# grab the layers\n",
        "layers = model.layers\n",
        "\n",
        "# Extracts the outputs of the top 8 layers:\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "# Creates a model that will return these outputs, given the model input:\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "# This will return a list of 5 Numpy arrays:\n",
        "# one array per layer activation\n",
        "activations = activation_model.predict(img_tensor)\n",
        "\n",
        "# first conv layer\n",
        "first_layer_activation = activations[2]\n",
        "print(first_layer_activation.shape)\n",
        "\n",
        "# visualize 3rd channel\n",
        "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
        "plt.show()\n",
        "\n",
        "# let's show all channels in each 8 activation maps\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "# Now let's display our feature maps\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # This is the number of features in the feature map\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # We will tile the activation channels in this matrix\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # We'll tile each filter into this big horizontal grid\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            # Post-process the feature to make it visually palatable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Display the grid\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ZP5cWZlJIDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Convnet Layers"
      ],
      "metadata": {
        "id": "CRrSSFKwJOZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disable_eager_execution()\n",
        "\n",
        "layer_name = 'conv1'\n",
        "filter_index = 0\n",
        "\n",
        "# The call to `gradients` returns a list of tensors (of size 1 in this case)\n",
        "# hence we only keep the first element -- which is a tensor.\n",
        "\n",
        "layer_output = model.get_layer(layer_name).output\n",
        "\n",
        "sys.setrecursionlimit(10000)\n",
        "loss = mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "# We add 1e-5 before dividing so as to avoid accidentally dividing by 0.\n",
        "grads /= (backend.sqrt(backend.mean(backend.square(grads))) + 1e-5)\n",
        "\n",
        "iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "# Let's test it:\n",
        "loss_value, grads_value = iterate([np.zeros((1, 64, 64, 3))])\n",
        "\n",
        "# We start from a gray image with some noise\n",
        "input_img_data = np.random.random((1, 64, 64, 3)) * 20 + 128.\n",
        "\n",
        "# Run gradient ascent for 40 steps\n",
        "step = 1.  # this is the magnitude of each gradient update\n",
        "for i in range(40):\n",
        "    # Compute the loss value and gradient value\n",
        "    loss_value, grads_value = iterate([input_img_data])\n",
        "    # Here we adjust the input image in the direction that maximizes the loss\n",
        "    input_img_data += grads_value * step\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def generate_pattern(layer_name, filter_index, size=150):\n",
        "    # Build a loss function that maximizes the activation\n",
        "    # of the nth filter of the layer considered.\n",
        "    layer_output = model.get_layer(layer_name).output\n",
        "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "    # Compute the gradient of the input picture wrt this loss\n",
        "    grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "    # Normalization trick: we normalize the gradient\n",
        "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "    # This function returns the loss and grads given the input picture\n",
        "    iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "    # We start from a gray image with some noise\n",
        "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
        "\n",
        "    # Run gradient ascent for 40 steps\n",
        "    step = 1.\n",
        "    for i in range(40):\n",
        "        loss_value, grads_value = iterate([input_img_data])\n",
        "        input_img_data += grads_value * step\n",
        "\n",
        "    img = input_img_data[0]\n",
        "    return deprocess_image(img)\n",
        "\n",
        "# show a conv layer\n",
        "plt.imshow(generate_pattern('res5c_branch2c', 0))\n",
        "plt.show()\n",
        "\n",
        "# show four layer\n",
        "for layer_name in ['res5b_branch2c', 'res5c_branch2a', 'res5c_branch2b', 'res5c_branch2c']:\n",
        "    size = 64\n",
        "    margin = 5\n",
        "\n",
        "    # This a empty (black) image where we will store our results.\n",
        "    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
        "\n",
        "    for i in range(8):  # iterate over the rows of our results grid\n",
        "        for j in range(8):  # iterate over the columns of our results grid\n",
        "            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
        "            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
        "\n",
        "            # Put the result in the square `(i, j)` of the results grid\n",
        "            horizontal_start = i * size + i * margin\n",
        "            horizontal_end = horizontal_start + size\n",
        "            vertical_start = j * size + j * margin\n",
        "            vertical_end = vertical_start + size\n",
        "            results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img\n",
        "\n",
        "    # Display the results grid\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.imshow((results * 255).astype(np.uint8))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i6IyH_TcJWmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Heat Map"
      ],
      "metadata": {
        "id": "hxNfm8v2JYD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a single image directory\n",
        "img_path = '/content/cifar10-64/test/class0/img10.png'\n",
        "\n",
        "# `img` is a PIL image of size 224x224\n",
        "img = load_img(img_path, target_size=(64, 64))\n",
        "\n",
        "# `x` is a float32 Numpy array of shape (64, 64, 3)\n",
        "x = img_to_array(img)\n",
        "\n",
        "# We add a dimension to transform our array into a \"batch\"\n",
        "# of size (1, 224, 224, 3)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Finally we preprocess the batch\n",
        "# (this does channel-wise color normalization)\n",
        "x = vgg16.preprocess_input(x)\n",
        "\n",
        "# prediction probabilities\n",
        "preds = model.predict(x)\n",
        "\n",
        "# This is the \"african elephant\" entry in the prediction vector\n",
        "airplane = model.output[:, 0]\n",
        "\n",
        "# The is the output feature map of the `block5_conv3` layer,\n",
        "# the last convolutional layer in VGG16\n",
        "last_conv_layer = model.get_layer('res5c_branch2c')\n",
        "\n",
        "ops.disable_eager_execution()\n",
        "\n",
        "# This is the gradient of the \"african elephant\" class with regard to\n",
        "# the output feature map of `block5_conv3`\n",
        "grads = K.gradients(airplane, last_conv_layer.output)[0]\n",
        "\n",
        "# This is a vector of shape (512,), where each entry\n",
        "# is the mean intensity of the gradient over a specific feature map channel\n",
        "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "# This function allows us to access the values of the quantities we just defined:\n",
        "# `pooled_grads` and the output feature map of `block5_conv3`,\n",
        "# given a sample image\n",
        "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "# These are the values of these two quantities, as Numpy arrays,\n",
        "# given our sample image of two elephants\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "# We multiply each channel in the feature map array\n",
        "# by \"how important this channel is\" with regard to the elephant class\n",
        "for i in range(512):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "# The channel-wise mean of the resulting feature map\n",
        "# is our heatmap of class activation\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "\n",
        "# normalize heat map between 0-1\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)\n",
        "plt.show()\n",
        "\n",
        "# We use cv2 to load the original image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# We resize the heatmap to have the same size as the original image\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# We convert the heatmap to RGB\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# We apply the heatmap to the original image\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "# 0.4 here is a heatmap intensity factor\n",
        "superimposed_img = heatmap * 0.4 + img\n",
        "\n",
        "# Save the image to disk\n",
        "cv2.imwrite('images/airplane.jpg', superimposed_img)"
      ],
      "metadata": {
        "id": "p0V56B3rFtyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NaTH6J77TAX"
      },
      "source": [
        "## PS 1.B - 20 points\n",
        "\n",
        "In [this notebook](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/transfer-learning/transfer_learning_tutorial.html) we showcase _transfer learning_ using a pre-trained CNN model.\n",
        "\n",
        "Perform the fine-tunning and feature extraction methods of transfer learning using the same model as in PS-1A, for the class `ship`.\n",
        "\n",
        "Repeat the visualization of PS-1.A before and after  transfer learning and write a conclusive summary as to the relative value of the two methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ctjlwi7TAY"
      },
      "outputs": [],
      "source": [
        "# Insert your code here"
      ]
    }
  ]
}